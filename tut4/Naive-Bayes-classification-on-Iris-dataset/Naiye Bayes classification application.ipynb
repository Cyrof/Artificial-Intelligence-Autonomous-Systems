{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Naive Bayes Classification\n", "\n", "# Importing the libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import matplotlib.image as mpimg\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Importing the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["dataset = pd.read_csv('iris.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Looking at the first 5 values of the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Displaying Image"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "img=mpimg.imread('iris_types.jpg')\n", "plt.figure(figsize=(20,40))\n", "plt.axis('off')\n", "plt.imshow(img)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Splitting the dataset in independent and dependent variables"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["X = dataset.iloc[:,:4].values\n", "y = dataset['species'].values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Splitting the dataset into the Training set and Test set "]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 82)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Feature Scaling to bring the variabele in a single scale"]}, {"cell_type": "code", "execution_count": null, "metadata": {"collapsed": true}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "sc = StandardScaler()\n", "X_train = sc.fit_transform(X_train)\n", "X_test = sc.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Custom Naive Bayes Classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "class CustomNaiveBayes:\n", "    def fit(self, X, y):\n", "        self.classes = np.unique(y)\n", "        self.mean = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)\n", "        self.var = np.zeros((len(self.classes), X.shape[1]), dtype=np.float64)\n", "        self.priors = np.zeros(len(self.classes), dtype=np.float64)\n", "        \n", "        for idx, c in enumerate(self.classes):\n", "            X_c = X[y == c]\n", "            self.mean[idx, :] = X_c.mean(axis=0)\n", "            self.var[idx, :] = X_c.var(axis=0)\n", "            self.priors[idx] = X_c.shape[0] / float(X.shape[0])\n", "    \n", "    def _calculate_prior(self, class_idx):\n", "        return np.log(self.priors[class_idx])\n", "    \n", "    def _calculate_likelihood(self, class_idx, x):\n", "        mean = self.mean[class_idx]\n", "        var = self.var[class_idx]\n", "        numerator = np.exp(- (x - mean) ** 2 / (2 * var))\n", "        denominator = np.sqrt(2 * np.pi * var)    \n", "        return np.sum(np.log(numerator / denominator))\n", "    \n", "    def _calculate_posterior(self, x):\n", "        posteriors = []\n", "        for idx, _ in enumerate(self.classes):\n", "            prior = self._calculate_prior(idx)\n", "            likelihood = self._calculate_likelihood(idx, x)\n", "            posterior = prior + likelihood\n", "            posteriors.append(posterior)\n", "        return self.classes[np.argmax(posteriors)]\n", "    \n", "    def predict(self, X):\n", "        y_pred = [self._calculate_posterior(x) for x in X]\n", "        return np.array(y_pred)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Instantiate and train the custom classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["custom_nb_classifier = CustomNaiveBayes()\n", "custom_nb_classifier.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Predicting the Test set results with Custom Naive Bayes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred_custom = custom_nb_classifier.predict(X_test)\n", "y_pred_custom"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing actual and predicted values "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_compare = np.vstack((y_test, y_pred_custom)).T\n", "# actual value on the left side and predicted value on the right side\n", "# printing the top 5 values\n", "y_compare[:5, :]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Making the Confusion Matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import confusion_matrix\n", "cm_custom = confusion_matrix(y_test, y_pred_custom)\n", "cm_custom"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Finding the accuracy from the confusion matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["a = cm_custom.shape\n", "corrPred = 0\n", "falsePred = 0\n", "\n", "for row in range(a[0]):\n", "    for c in range(a[1]):\n", "        if row == c:\n", "            corrPred += cm_custom[row, c]\n", "        else: \n", "            falsePred += cm_custom[row, c]\n", "\n", "print(f\"Correct predictions: {corrPred}\")\n", "print(f\"False predictions: {falsePred}\")\n", "print(f\"\\n\\nAccuracy of the Custom Naive Bayes Classifier is: {corrPred/cm_custom.sum()}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparing with GaussianNB"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fitting Naive Bayes Classification to the Training set with linear kernel\n", "from sklearn.naive_bayes import GaussianNB\n", "nvclassifier = GaussianNB()\n", "nvclassifier.fit(X_train, y_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Predicting the Test set results\n", "y_pred = nvclassifier.predict(X_test)\n", "print(y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#lets see the actual and predicted value side by side\n", "y_compare = np.vstack((y_test,y_pred)).T\n", "#actual value on the left side and predicted value on the right hand side\n", "#printing the top 5 values\n", "y_compare[:5,:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Making the Confusion Matrix\n", "from sklearn.metrics import confusion_matrix\n", "cm = confusion_matrix(y_test, y_pred)\n", "print(cm)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#finding accuracy from the confusion matrix.\n", "a = cm.shape\n", "corrPred = 0\n", "falsePred = 0\n", "\n", "for row in range(a[0]):\n", "    for c in range(a[1]):\n", "        if row == c:\n", "            corrPred +=cm[row,c]\n", "        else:\n", "            falsePred += cm[row,c]\n", "print('Correct predictions: ', corrPred)\n", "print('False predictions', falsePred)\n", "print ('\\n\\nAccuracy of the Naive Bayes Clasification is: ', corrPred/(cm.sum()))            \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Comparison of Custom Naive Bayes Classifier and `GaussianNB`\n", "Both the custom Naive Bayes classifier and the `GaussianNB` classifier from `scikit-learn` showed the same performance on the Iris dataset in terms of accuracy and prediction results. This outcome can be attributed to the following reasons: \n", "\n", "1. **Underlying Algorithm:**\n", "   - Both classifiers are based on the Gaussian Naive Bayes algorithm. They calculate the prior probability and conditional probability asusming a Gaussian (normal) distribution for the features. This results in similar probabilistic computations for classification.\n", "\n", "2. **Mathematical Equivalence:**\n", "   - The custom implementation of the Naive Bayes classifier follows the same mathematical principles as `GaussianNB`. The calculation of mean, variance, and likelihood of features given the class, and the computation of posterior probabilities, are fundamentally the same in both implementations.\n", "\n", "3. **Feature Scaling:**\n", "   - Feature scaling is applied to both classifiers using `StandardScaler` from `scikit-learn`. This ensures that the features are normalised, resulting in similar input data for both classifiers, which contributes to the same classification results.\n", "\n", "4. **Data Splitting:**\n", "   - The same training and test datasets are used for both classifiers. The `train_test_split` function from `scikit-learn` ensures that both classifiers are trained and tested on identical data, leading to comparable performance.\n", "\n", "5. **Implementation Accuracy**\n", "   - The custom Naive Bayes classifier is implemented correctly, adhering to the theoretical principles of Gaussian Naive Bayes. As long as the implementation is correct, the results should match those of the optimised `GaussianNB` provided by `scikit-learn`.\n", "\n", "### Conclusion\n", "WHile the custom Naive Bayes classifier helps in understanding the underlying mechanics of the algorithm, using the `GaussianNB` from `scikit-learn` is generally preferable for practical applications due to its optimisation and robustness. However, the matching performance demonstrates the correctness of the custom implementaion and reinforces the theoretical concepts of the Gaussian Naive Bayes classifier."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 2}
